{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load YouTube API key\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the channel ID's from the channel names\n",
    "def get_channel_ids(channel_names):\n",
    "    channel_data = []\n",
    "    \n",
    "    for channel_name in channel_names:\n",
    "        request = youtube.search().list(\n",
    "            q=channel_name,\n",
    "            type='channel',\n",
    "            part='id',\n",
    "            maxResults=1\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        if 'items' in response:\n",
    "            channel_id = response['items'][0]['id']['channelId']\n",
    "            channel_data.append({'channel_name': channel_name, 'channel_id': channel_id})\n",
    "        else:\n",
    "            print(f\"Could not find channel ID for '{channel_name}'\")\n",
    "    \n",
    "    df_channels = pd.DataFrame(channel_data)\n",
    "    return df_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           channel_name                channel_id\n",
      "0         TheArmbandFPL  UC4UdmU9tNnU5iQVmQB3Ngvg\n",
      "1              elitefpl  UCOhHIQyQg4dNKvWg0tg12zg\n",
      "2  fantasyfootballfixYT  UC0Oaf88gRGnNkncI8D_GO-Q\n",
      "3              FFScout_  UCKxYKQ8pgJ7V8wwh4hLsSXQ\n",
      "4       AboveAverageFPL  UCnaJiRMf5hju0TlaeGK5CDQ\n",
      "5             fplbanger  UC1dzUZYYluvh8ktUYFYk8PA\n",
      "6           fplblackbox  UCGJ8-xqhOLwyJNuPMsVoQWQ\n",
      "7              FPLFocal  UC72QokPHXQ9r98ROfNZmaDw\n"
     ]
    }
   ],
   "source": [
    "channel_names = [\n",
    "    \"TheArmbandFPL\",\n",
    "    \"elitefpl\",\n",
    "    \"fantasyfootballfixYT\",\n",
    "    \"FFScout_\",\n",
    "    \"AboveAverageFPL\",\n",
    "    \"fplbanger\",\n",
    "    \"fplblackbox\",\n",
    "    \"FPLFocal\"\n",
    "]\n",
    "\n",
    "channel_ids = df_channels = get_channel_ids(channel_names)\n",
    "\n",
    "print(channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the most recent video IDs from a channel\n",
    "def get_channel_videos(df_channels, published_after, max_results=10):\n",
    "    all_videos = []\n",
    "    \n",
    "    for _, row in df_channels.iterrows():\n",
    "        channel_id = row['channel_id']\n",
    "        channel_name = row['channel_name']\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        while True:\n",
    "            request = youtube.search().list(\n",
    "                part=\"id,snippet\",\n",
    "                channelId=channel_id,\n",
    "                type=\"video\",\n",
    "                order=\"date\",\n",
    "                publishedAfter=published_after,\n",
    "                maxResults=max_results,\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                video_id = item['id']['videoId']\n",
    "                title = item['snippet']['title']\n",
    "                published_at = item['snippet']['publishedAt']\n",
    "                videos.append({\n",
    "                    'channel_name': channel_name,\n",
    "                    'id': video_id,\n",
    "                    'title': title,\n",
    "                    'published_at': published_at\n",
    "                })\n",
    "                \n",
    "                if len(videos) >= max_results:\n",
    "                    break\n",
    "            \n",
    "            if len(videos) >= max_results:\n",
    "                break\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "        \n",
    "        all_videos.extend(videos)\n",
    "    \n",
    "    return pd.DataFrame(all_videos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            channel_name           id  \\\n",
      "0          TheArmbandFPL  Ah-t8j0wmsQ   \n",
      "1          TheArmbandFPL  hlWD3N7iohY   \n",
      "2          TheArmbandFPL  RtkI_Txs8zk   \n",
      "3          TheArmbandFPL  dAZs-gQkk8A   \n",
      "4          TheArmbandFPL  xQVDXFka6e8   \n",
      "5               elitefpl  r3VkzwaZnSE   \n",
      "6               elitefpl  tZOBHSM-g5Q   \n",
      "7               elitefpl  Fvgtyurnu8k   \n",
      "8               elitefpl  Xy2_A7r1yKA   \n",
      "9               elitefpl  qbcn_YvFbkI   \n",
      "10  fantasyfootballfixYT  JcNY6U3xW7A   \n",
      "11  fantasyfootballfixYT  4nszN8HLqtI   \n",
      "12  fantasyfootballfixYT  yPDDg71oLxs   \n",
      "13  fantasyfootballfixYT  EUwZIkxvLtA   \n",
      "14  fantasyfootballfixYT  9H5v1nH6pAY   \n",
      "15              FFScout_  rJ6v57K7sgQ   \n",
      "16              FFScout_  W9xLUbv_-_o   \n",
      "17              FFScout_  sqQX_ZPoc0g   \n",
      "18              FFScout_  zf44PnRKGig   \n",
      "19              FFScout_  ZNU7iPNPGMg   \n",
      "20       AboveAverageFPL  mEm14WbHwXc   \n",
      "21       AboveAverageFPL  rVXFYd-BB98   \n",
      "22       AboveAverageFPL  oFHBu_Hyn4I   \n",
      "23       AboveAverageFPL  2A4NgVvpHKU   \n",
      "24       AboveAverageFPL  NyRgdmiMU_8   \n",
      "25             fplbanger  q0RzTVclWcI   \n",
      "26             fplbanger  C-fGfqMK28E   \n",
      "27             fplbanger  cwm875XUXag   \n",
      "28             fplbanger  vaW7z3eEUlQ   \n",
      "29             fplbanger  L2UCn85HcV4   \n",
      "30           fplblackbox  O2mKwgFGSIk   \n",
      "31           fplblackbox  h4UiCLP4SEQ   \n",
      "32           fplblackbox  4NCHTE4ugK4   \n",
      "33           fplblackbox  QEPCs94LVac   \n",
      "34           fplblackbox  U7A6-WjhO9s   \n",
      "35              FPLFocal  umfuc1ibh4k   \n",
      "36              FPLFocal  3uaXHvTGuWA   \n",
      "37              FPLFocal  EO2ZRvv5W0Y   \n",
      "38              FPLFocal  O5OSdySZ7N0   \n",
      "39              FPLFocal  GMmfngcIfHI   \n",
      "\n",
      "                                                title          published_at  \n",
      "0    FPL GW3 PREVIEW | PALMER BANDWAGON | FPL ARMBAND  2024-08-29T05:03:00Z  \n",
      "1   FPL GW2 PREVIEW | HAALAND v IPSWICH | FPL ARMBAND  2024-08-22T05:11:46Z  \n",
      "2   GW1 TEAM REVEALS | SALAH ONLY? HAALAND ONLY! |...  2024-08-16T05:41:11Z  \n",
      "3   PRE-SEASON PICKS &amp; DRAFTS | HOT OR NOT? | ...  2024-08-09T06:04:17Z  \n",
      "4   FPL PRICE CHANGES 24/25 | FIRST DRAFT | NEW RU...  2024-07-18T05:02:30Z  \n",
      "5   FPL GAMEWEEK 4 WILDCARD DRAFT | THE BIG DEBATE...  2024-09-05T21:10:41Z  \n",
      "6   FPL GAMEWEEK 4 SUNDAY SURGERY | LET&#39;S RE-E...  2024-09-01T20:40:54Z  \n",
      "7   Alex Vollo Captained Morgan Gibbs White for 1p...  2024-08-31T20:46:11Z  \n",
      "8   FPL GAMEWEEK 3 MATCH REACTION | &quot;OHHHHHHH...  2024-08-31T20:33:06Z  \n",
      "9   Kepa to Crystal Palace ðŸ¤” &quot;Steve-O he move...  2024-08-29T11:36:37Z  \n",
      "10  ðŸ¤– AI WILDCARD FPL TEAM REVEAL GAMEWEEK 4 | Edd...  2024-09-06T12:00:31Z  \n",
      "11  ðŸ”¥ GW4 WILDCARD TEAM: Salah, Haaland &amp; Top ...  2024-09-05T10:00:04Z  \n",
      "12  ðŸ¤– AI FPL BEST TEAM REVEAL &amp; Top Captain Pi...  2024-08-30T12:01:10Z  \n",
      "13  âŒ IS MADUEKE AN FPL TRAP? | TOP Fantasy Premie...  2024-08-28T11:00:21Z  \n",
      "14  ðŸ”¥ FPL TEAM REVEAL | Nkunku OUT? | Fantasy Prem...  2024-08-26T11:00:12Z  \n",
      "15  FPL WILDCARD DRAFT GW4 ðŸ™Œ | NO HAALAND! ðŸ˜± | Fan...  2024-09-06T16:30:09Z  \n",
      "16  FPL GW4 Q&amp;A! â‰ï¸ | AZ AND SAM! | Fantasy Pr...  2024-09-05T18:35:50Z  \n",
      "17  FPL BURNING QUESTIONS GW4! ðŸ”¥ | BEST CHEAP PLAY...  2024-09-04T12:27:42Z  \n",
      "18  FPL SCOUTCAST GW4! ðŸš¨ | WILDCARD SPECIAL! | Fan...  2024-09-03T21:33:35Z  \n",
      "19  TOM F&#39;S GW4 FPL TRANSFER PLANS! ðŸ’¥ | BUY WA...  2024-09-03T12:40:00Z  \n",
      "20  Haaland to break all records? Liverpool for th...  2024-09-08T09:33:56Z  \n",
      "21  FPL WILDCARD ACTIVE w/ @LetsTalkFPL | Fantasy ...  2024-09-04T18:27:04Z  \n",
      "22  FPL GW3 Reaction - Boom Diaz!! | Fantasy Premi...  2024-09-01T21:27:37Z  \n",
      "23  FPL GW3 Team Selection | Fantasy Premier Leagu...  2024-08-29T18:16:03Z  \n",
      "24  Team Management Secrets: Adam&#39;s Man City P...  2024-08-28T12:56:40Z  \n",
      "25                                     GW3 Wildcard??  2024-08-30T01:21:56Z  \n",
      "26  Is Haaland worth 15m ? When you can have Palme...  2024-08-29T06:16:15Z  \n",
      "27   FPL GW3: WILDCARD ACTIVATED!  - FPL Tips 2024/25  2024-08-28T13:14:05Z  \n",
      "28  Triple Captain Haaland is a perfectly logical ...  2024-08-23T09:35:51Z  \n",
      "29  FPL GW2 TEAM REVEAL : RICO IN! - FPL Tips 2024/25  2024-08-22T09:24:37Z  \n",
      "30  FPL BlackBox | Roll With It | Fantasy Premier ...  2024-08-28T22:42:53Z  \n",
      "31                              FPL Wissa Time? | GW3  2024-08-26T23:31:24Z  \n",
      "32  MOTD Challenge | Fantasy Premier League Tips 2...  2024-08-24T23:16:38Z  \n",
      "33  FPL BlackBox | In Our Defence | Fantasy Premie...  2024-08-22T22:20:50Z  \n",
      "34                         FPL Quansah Quandary | GW2  2024-08-20T20:36:01Z  \n",
      "35                          FPL WILDCARD TEAM - GW4 âœ…  2024-09-04T09:01:01Z  \n",
      "36                          FPL WILDCARD TEAM | GW4 âœ…  2024-09-03T11:08:19Z  \n",
      "37  FPL GW4 TRANSFER PLANS | 8 THINGS WE&#39;VE LE...  2024-09-01T17:56:28Z  \n",
      "38               FPL DEADLINE STREAM GW3 ðŸš¨ TEAM NEWS!  2024-08-31T10:15:37Z  \n",
      "39            FINAL FPL TIPS | GW3 | DEADLINE NEWS âš ï¸  2024-08-30T12:22:51Z  \n"
     ]
    }
   ],
   "source": [
    "# Extract the last n video ID's for each of the youtube accounts\n",
    "published_after = \"2023-09-01T00:00:00Z\" \n",
    "df_videos = get_channel_videos(df_channels, published_after, max_results=5)\n",
    "\n",
    "print(df_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grab the transcripts if they exist\n",
    "def get_transcripts(df_videos):\n",
    "    transcripts = []\n",
    "    for _, row in df_videos.iterrows():\n",
    "        video_id = row['id']\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            full_transcript = ' '.join([entry['text'] for entry in transcript])\n",
    "            transcripts.append(full_transcript)\n",
    "        except Exception as e:\n",
    "            transcripts.append(None)\n",
    "            print(f\"Error getting transcript for video {video_id}: {str(e)}\")\n",
    "    return transcripts\n",
    "\n",
    "# Function to calculate word count\n",
    "def word_count(transcript):\n",
    "    if transcript:\n",
    "        return len(transcript.split())  # Split by whitespace to count words\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting transcript for video mEm14WbHwXc: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=mEm14WbHwXc! This is most likely caused by:\n",
      "\n",
      "Subtitles are disabled for this video\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "            channel_name           id  \\\n",
      "0          TheArmbandFPL  Ah-t8j0wmsQ   \n",
      "1          TheArmbandFPL  hlWD3N7iohY   \n",
      "2          TheArmbandFPL  RtkI_Txs8zk   \n",
      "3          TheArmbandFPL  dAZs-gQkk8A   \n",
      "4          TheArmbandFPL  xQVDXFka6e8   \n",
      "5               elitefpl  r3VkzwaZnSE   \n",
      "6               elitefpl  tZOBHSM-g5Q   \n",
      "7               elitefpl  Fvgtyurnu8k   \n",
      "8               elitefpl  Xy2_A7r1yKA   \n",
      "9               elitefpl  qbcn_YvFbkI   \n",
      "10  fantasyfootballfixYT  JcNY6U3xW7A   \n",
      "11  fantasyfootballfixYT  4nszN8HLqtI   \n",
      "12  fantasyfootballfixYT  yPDDg71oLxs   \n",
      "13  fantasyfootballfixYT  EUwZIkxvLtA   \n",
      "14  fantasyfootballfixYT  9H5v1nH6pAY   \n",
      "15              FFScout_  rJ6v57K7sgQ   \n",
      "16              FFScout_  W9xLUbv_-_o   \n",
      "17              FFScout_  sqQX_ZPoc0g   \n",
      "18              FFScout_  zf44PnRKGig   \n",
      "19              FFScout_  ZNU7iPNPGMg   \n",
      "20       AboveAverageFPL  mEm14WbHwXc   \n",
      "21       AboveAverageFPL  rVXFYd-BB98   \n",
      "22       AboveAverageFPL  oFHBu_Hyn4I   \n",
      "23       AboveAverageFPL  2A4NgVvpHKU   \n",
      "24       AboveAverageFPL  NyRgdmiMU_8   \n",
      "25             fplbanger  q0RzTVclWcI   \n",
      "26             fplbanger  C-fGfqMK28E   \n",
      "27             fplbanger  cwm875XUXag   \n",
      "28             fplbanger  vaW7z3eEUlQ   \n",
      "29             fplbanger  L2UCn85HcV4   \n",
      "30           fplblackbox  O2mKwgFGSIk   \n",
      "31           fplblackbox  h4UiCLP4SEQ   \n",
      "32           fplblackbox  4NCHTE4ugK4   \n",
      "33           fplblackbox  QEPCs94LVac   \n",
      "34           fplblackbox  U7A6-WjhO9s   \n",
      "35              FPLFocal  umfuc1ibh4k   \n",
      "36              FPLFocal  3uaXHvTGuWA   \n",
      "37              FPLFocal  EO2ZRvv5W0Y   \n",
      "38              FPLFocal  O5OSdySZ7N0   \n",
      "39              FPLFocal  GMmfngcIfHI   \n",
      "\n",
      "                                                title  word_count  \\\n",
      "0    FPL GW3 PREVIEW | PALMER BANDWAGON | FPL ARMBAND       10355   \n",
      "1   FPL GW2 PREVIEW | HAALAND v IPSWICH | FPL ARMBAND       10461   \n",
      "2   GW1 TEAM REVEALS | SALAH ONLY? HAALAND ONLY! |...        9316   \n",
      "3   PRE-SEASON PICKS &amp; DRAFTS | HOT OR NOT? | ...       11488   \n",
      "4   FPL PRICE CHANGES 24/25 | FIRST DRAFT | NEW RU...        8926   \n",
      "5   FPL GAMEWEEK 4 WILDCARD DRAFT | THE BIG DEBATE...        9695   \n",
      "6   FPL GAMEWEEK 4 SUNDAY SURGERY | LET&#39;S RE-E...        9184   \n",
      "7   Alex Vollo Captained Morgan Gibbs White for 1p...          74   \n",
      "8   FPL GAMEWEEK 3 MATCH REACTION | &quot;OHHHHHHH...       11875   \n",
      "9   Kepa to Crystal Palace ðŸ¤” &quot;Steve-O he move...          18   \n",
      "10  ðŸ¤– AI WILDCARD FPL TEAM REVEAL GAMEWEEK 4 | Edd...        3553   \n",
      "11  ðŸ”¥ GW4 WILDCARD TEAM: Salah, Haaland &amp; Top ...        2772   \n",
      "12  ðŸ¤– AI FPL BEST TEAM REVEAL &amp; Top Captain Pi...        3451   \n",
      "13  âŒ IS MADUEKE AN FPL TRAP? | TOP Fantasy Premie...        2281   \n",
      "14  ðŸ”¥ FPL TEAM REVEAL | Nkunku OUT? | Fantasy Prem...        5469   \n",
      "15  FPL WILDCARD DRAFT GW4 ðŸ™Œ | NO HAALAND! ðŸ˜± | Fan...        5000   \n",
      "16  FPL GW4 Q&amp;A! â‰ï¸ | AZ AND SAM! | Fantasy Pr...        9225   \n",
      "17  FPL BURNING QUESTIONS GW4! ðŸ”¥ | BEST CHEAP PLAY...       12982   \n",
      "18  FPL SCOUTCAST GW4! ðŸš¨ | WILDCARD SPECIAL! | Fan...       12052   \n",
      "19  TOM F&#39;S GW4 FPL TRANSFER PLANS! ðŸ’¥ | BUY WA...        5198   \n",
      "20  Haaland to break all records? Liverpool for th...           0   \n",
      "21  FPL WILDCARD ACTIVE w/ @LetsTalkFPL | Fantasy ...       16072   \n",
      "22  FPL GW3 Reaction - Boom Diaz!! | Fantasy Premi...       17790   \n",
      "23  FPL GW3 Team Selection | Fantasy Premier Leagu...        9968   \n",
      "24  Team Management Secrets: Adam&#39;s Man City P...         190   \n",
      "25                                     GW3 Wildcard??          91   \n",
      "26  Is Haaland worth 15m ? When you can have Palme...         173   \n",
      "27   FPL GW3: WILDCARD ACTIVATED!  - FPL Tips 2024/25       11536   \n",
      "28  Triple Captain Haaland is a perfectly logical ...         184   \n",
      "29  FPL GW2 TEAM REVEAL : RICO IN! - FPL Tips 2024/25        3850   \n",
      "30  FPL BlackBox | Roll With It | Fantasy Premier ...       32090   \n",
      "31                              FPL Wissa Time? | GW3        3455   \n",
      "32  MOTD Challenge | Fantasy Premier League Tips 2...       18913   \n",
      "33  FPL BlackBox | In Our Defence | Fantasy Premie...       27302   \n",
      "34                         FPL Quansah Quandary | GW2        3894   \n",
      "35                          FPL WILDCARD TEAM - GW4 âœ…         199   \n",
      "36                          FPL WILDCARD TEAM | GW4 âœ…        2107   \n",
      "37  FPL GW4 TRANSFER PLANS | 8 THINGS WE&#39;VE LE...        2197   \n",
      "38               FPL DEADLINE STREAM GW3 ðŸš¨ TEAM NEWS!       20749   \n",
      "39            FINAL FPL TIPS | GW3 | DEADLINE NEWS âš ï¸        2021   \n",
      "\n",
      "                                           transcript  \n",
      "0   [Music] n [Music] [Music] [Music] [Music] hell...  \n",
      "1   done it from the like um this one that's alrea...  \n",
      "2   [Music] [Music] he [Music] [Music] [Music] hel...  \n",
      "3   [Music] [Music] [Music] [Music] hello everyone...  \n",
      "4   [Music] [Music] he [Music] a [Music] [Music] h...  \n",
      "5   a impromptu I guess live stream uh discussing ...  \n",
      "6   all right let's go I'm running away from the p...  \n",
      "7   [Music] [Applause] [Music] [Applause] [Music] ...  \n",
      "8   from my past I left behind I'm looking for ans...  \n",
      "9   no no one look at this this this Sports Guru d...  \n",
      "10  hello everybody and welcome back to the fantas...  \n",
      "11  yes guys welcome back to another fantasy footb...  \n",
      "12  hello everybody and welcome back to the fantas...  \n",
      "13  yes guys welcome back to another fancy footbal...  \n",
      "14  hi everyone and welcome back to the fantasy fo...  \n",
      "15  [Music] [Applause] hello and welcome to anothe...  \n",
      "16  [Applause] good afternoon everybody Welcome to...  \n",
      "17  [Applause] good afternoon what is going on eve...  \n",
      "18  [Music] [Applause] hello hello hello and welco...  \n",
      "19  [Music] [Applause] hello and welcome to anothe...  \n",
      "20                                               None  \n",
      "21  three game weeks in and we're now into the fir...  \n",
      "22  [Music] n n n n [Music] n n n n [Music] right ...  \n",
      "23  oh [Music] right guys welcome back this is the...  \n",
      "24  I can't bring myself to do it because I can't ...  \n",
      "25  I think at this stage you know if you can stom...  \n",
      "26  I would say that Holland plus Morgan Rogers eq...  \n",
      "27  I think at this stage you know if you can stom...  \n",
      "28  is to look at a single game week for triple ca...  \n",
      "29  oh this thing hello everyone and welcome back ...  \n",
      "30  [Music] [Applause] [Music] [Music] good evenin...  \n",
      "31  [Music] so game week two it's match of the day...  \n",
      "32  [Applause] [Music] [Music] we're here hello ev...  \n",
      "33  [Music] [Music] [Music] [Applause] [Music] [Mu...  \n",
      "34  [Music] so at least three or four people said ...  \n",
      "35  this is a wild card team for gami 4 to smash i...  \n",
      "36  welcome back for another video we're in an int...  \n",
      "37  welcome back for another video gamei 3 is just...  \n",
      "38  [Music] [Music] [Music] you [Music] the waves ...  \n",
      "39  welcome back for another video we're going to ...  \n"
     ]
    }
   ],
   "source": [
    "# Fetch transcripts for df_videos\n",
    "transcripts = get_transcripts(df_videos)\n",
    "\n",
    "# Add transcripts to the DataFrame\n",
    "df_videos['transcript'] = transcripts\n",
    "\n",
    "# Add word count of the transcripts\n",
    "df_videos['word_count'] = df_videos['transcript'].apply(word_count)\n",
    "\n",
    "# Only consider transripts with more than 1000 words\n",
    "df_videos = df_videos[df_videos['word_count'] >= 1000]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_videos[['channel_name', 'id', 'title', 'word_count', 'transcript']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split transcript into manageable chunks based on token count\n",
    "def split_into_chunks(transcript, max_tokens=4000):\n",
    "    # Tokenize the transcript\n",
    "    doc = nlp(transcript)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sentence_tokens = len(sentence.orth_.split())\n",
    "\n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            # If the current chunk exceeds the limit, start a new one\n",
    "            chunks.append(' '.join([str(sent) for sent in current_chunk]))\n",
    "            current_chunk = [sentence.text]\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current_chunk.append(sentence.text)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Append any remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join([str(sent) for sent in current_chunk]))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Preprocess transcripts to reduce token count\n",
    "def preprocess_transcript(text, max_tokens = 30000):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove text in square brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Remove filler words\n",
    "    filler_words = r'\\b(um|uh|like|you know|i mean|sort of|kind of)\\b'\n",
    "    text = re.sub(filler_words, '', text)\n",
    "    \n",
    "    # Remove repeated words\n",
    "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "    \n",
    "    # Remove speaker identifications (assuming format \"Name:\")\n",
    "    text = re.sub(r'\\w+:', '', text)\n",
    "    \n",
    "    # Simplify large numbers\n",
    "    text = re.sub(r'\\b(\\d+) thousand\\b', r'\\1k', text)\n",
    "    text = re.sub(r'\\b(\\d+) million\\b', r'\\1m', text)\n",
    "    \n",
    "    # Remove unnecessary punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Use abbreviations for common terms\n",
    "    abbreviations = {\n",
    "        'fantasy premier league': 'fpl',\n",
    "        'gameweek': 'gw',\n",
    "        'manchester united': 'man utd',\n",
    "        'manchester city': 'man city'\n",
    "    }\n",
    "    for full, abbr in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + full + r'\\b', abbr, text)\n",
    "    \n",
    "    # Simplify season references\n",
    "    text = re.sub(r'\\d{4}/\\d{4}\\s+season', 'last season', text)\n",
    "    \n",
    "    # Simplify player names (example for Haaland)\n",
    "    text = re.sub(r'\\berling haaland\\b', 'haaland', text)\n",
    "    \n",
    "    # Remove or simplify time references\n",
    "    text = re.sub(r'\\b\\d+ minutes left\\b', 'almost out of time', text)\n",
    "    \n",
    "    # Remove references to external content\n",
    "    text = re.sub(r'(check out|tune in to|watch) .+', '', text)\n",
    "    \n",
    "    return text #split_into_chunks(text, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos['transcript_chunks'] = df_videos['transcript'].apply(preprocess_transcript)\n",
    "df_videos['word_count_process'] = df_videos['transcript_chunks'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          channel_name_  word_count_mean  word_count_median  word_count_min  \\\n",
      "1              FFScout_      8891.400000             9225.0            5000   \n",
      "3         TheArmbandFPL     10109.200000            10355.0            8926   \n",
      "5  fantasyfootballfixYT      3505.200000             3451.0            2281   \n",
      "7           fplblackbox     17130.800000            18913.0            3455   \n",
      "2              FPLFocal      6768.500000             2152.0            2021   \n",
      "0       AboveAverageFPL     14610.000000            16072.0            9968   \n",
      "4              elitefpl     10251.333333             9695.0            9184   \n",
      "6             fplbanger      7693.000000             7693.0            3850   \n",
      "\n",
      "   word_count_max  word_count_sum  num_videos  word_count_process_mean  \\\n",
      "1           12982           44457           5                  2130.40   \n",
      "3           11488           50546           5                  6679.40   \n",
      "5            5469           17526           5                  2739.20   \n",
      "7           32090           85654           5                  1638.20   \n",
      "2           20749           27074           4                  1226.25   \n",
      "0           17790           43830           3                  8609.00   \n",
      "4           11875           30754           3                  5319.00   \n",
      "6           11536           15386           2                  4775.00   \n",
      "\n",
      "   word_count_process_median  word_count_process_min  word_count_process_max  \\\n",
      "1                     1427.0                     396                    4304   \n",
      "3                     8129.0                    3087                   10451   \n",
      "5                     2741.0                    2254                    3466   \n",
      "7                      830.0                     132                    4232   \n",
      "2                     1229.5                     257                    2189   \n",
      "0                     7736.0                    3009                   15082   \n",
      "4                     6101.0                     559                    9297   \n",
      "6                     4775.0                     698                    8852   \n",
      "\n",
      "   word_count_process_sum  percent_reduction  \n",
      "1                   10652          76.039769  \n",
      "3                   33397          33.927512  \n",
      "5                   13696          21.853247  \n",
      "7                    8191          90.437107  \n",
      "2                    4905          81.882987  \n",
      "0                   25827          41.074606  \n",
      "4                   15957          48.114066  \n",
      "6                    9550          37.930586  \n"
     ]
    }
   ],
   "source": [
    "# Summary of processing reduction effeciency\n",
    "summary = df_videos.groupby('channel_name').agg({\n",
    "    'word_count': ['mean', 'median', 'min', 'max', 'sum', 'count'],\n",
    "    'word_count_process': ['mean', 'median', 'min', 'max', 'sum']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the column names\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "\n",
    "# Rename the count column to 'num_videos'\n",
    "summary = summary.rename(columns={'word_count_count': 'num_videos'})\n",
    "\n",
    "# Calculate the percentage reduction in word count\n",
    "summary['percent_reduction'] = (1 - summary['word_count_process_sum'] / summary['word_count_sum']) * 100\n",
    "\n",
    "# Sort by number of videos, descending\n",
    "summary = summary.sort_values('num_videos', ascending=False)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_videos.to_csv(f'../output/transcripts_{published_after}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: TheArmbandFPL_2024-08-29T05:03:00Z.csv\n",
      "Saved: TheArmbandFPL_2024-08-22T05:11:46Z.csv\n",
      "Saved: TheArmbandFPL_2024-08-16T05:41:11Z.csv\n",
      "Saved: TheArmbandFPL_2024-08-09T06:04:17Z.csv\n",
      "Saved: TheArmbandFPL_2024-07-18T05:02:30Z.csv\n",
      "Saved: elitefpl_2024-09-05T21:10:41Z.csv\n",
      "Saved: elitefpl_2024-09-01T20:40:54Z.csv\n",
      "Saved: elitefpl_2024-08-31T20:33:06Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-06T12:00:31Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-05T10:00:04Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-08-30T12:01:10Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-08-28T11:00:21Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-08-26T11:00:12Z.csv\n",
      "Saved: FFScout__2024-09-06T16:30:09Z.csv\n",
      "Saved: FFScout__2024-09-05T18:35:50Z.csv\n",
      "Saved: FFScout__2024-09-04T12:27:42Z.csv\n",
      "Saved: FFScout__2024-09-03T21:33:35Z.csv\n",
      "Saved: FFScout__2024-09-03T12:40:00Z.csv\n",
      "Saved: AboveAverageFPL_2024-09-04T18:27:04Z.csv\n",
      "Saved: AboveAverageFPL_2024-09-01T21:27:37Z.csv\n",
      "Saved: AboveAverageFPL_2024-08-29T18:16:03Z.csv\n",
      "Saved: fplbanger_2024-08-28T13:14:05Z.csv\n",
      "Saved: fplbanger_2024-08-22T09:24:37Z.csv\n",
      "Saved: fplblackbox_2024-08-28T22:42:53Z.csv\n",
      "Saved: fplblackbox_2024-08-26T23:31:24Z.csv\n",
      "Saved: fplblackbox_2024-08-24T23:16:38Z.csv\n",
      "Saved: fplblackbox_2024-08-22T22:20:50Z.csv\n",
      "Saved: fplblackbox_2024-08-20T20:36:01Z.csv\n",
      "Saved: FPLFocal_2024-09-03T11:08:19Z.csv\n",
      "Saved: FPLFocal_2024-09-01T17:56:28Z.csv\n",
      "Saved: FPLFocal_2024-08-31T10:15:37Z.csv\n",
      "Saved: FPLFocal_2024-08-30T12:22:51Z.csv\n"
     ]
    }
   ],
   "source": [
    "def save_transcripts_as_csv(df, output_dir):\n",
    "    \"\"\"\n",
    "    Save transcripts from DataFrame as individual CSV files\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the transcripts\n",
    "    output_dir (str): Directory to save the CSV files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the DataFrame has the necessary columns\n",
    "    required_columns = ['channel_name', 'published_at', 'transcript_chunks']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame is missing one or more required columns: {required_columns}\")\n",
    "    \n",
    "    # Iterate through the DataFrame and save each transcript as a CSV file\n",
    "    for _, row in df.iterrows():\n",
    "        channel_name = row['channel_name']\n",
    "        episode_number = row['published_at']\n",
    "        transcript = row['transcript_chunks']\n",
    "        \n",
    "        # Create the filename\n",
    "        filename = f\"{channel_name}_{episode_number}.csv\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Create a new DataFrame with just the transcript\n",
    "        transcript_df = pd.DataFrame({'transcript': [transcript]})\n",
    "        \n",
    "        # Save the transcript as a CSV file\n",
    "        transcript_df.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "output_directory = \"../output/\"\n",
    "save_transcripts_as_csv(df_videos, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "def summarize_with_bart(text, max_length=150, min_length=50):\n",
    "    # Check if MPS is available and set the device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        # Load pre-trained model and tokenizer\n",
    "        model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "        tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
    "        \n",
    "        # Move input tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate summary\n",
    "        summary_ids = model.generate(inputs['input_ids'],\n",
    "                                     num_beams=4,\n",
    "                                     max_length=max_length,\n",
    "                                     min_length=min_length,\n",
    "                                     length_penalty=2.0,\n",
    "                                     early_stopping=True)\n",
    "\n",
    "        # Decode the generated summary\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during summarization: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_videos\n",
    "summary = summarize_with_bart(df['transcript_chunks'])\n",
    "if summary:\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"Summarization failed. Please check the error message above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "anthropic = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def summarize_transcript(transcript):\n",
    "    \"\"\"\n",
    "    Summarize a single transcript using Claude API\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Please summarize the following Fantasy Premier League podcast transcript. \n",
    "    Focus on the key points, player recommendations, and strategy advice.\n",
    "    Limit the summary to 3-5 bullet points.\n",
    "\n",
    "    Transcript:\n",
    "    {transcript}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    response = anthropic.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=\"You are an expert in Fantasy Premier League and podcast summarization.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.content\n",
    "\n",
    "def process_transcripts(transcripts_dir):\n",
    "    \"\"\"\n",
    "    Process all transcripts in the given directory and group summaries by channel name\n",
    "    \"\"\"\n",
    "    summaries = defaultdict(list)\n",
    "\n",
    "    for filename in os.listdir(transcripts_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            channel_name = filename.split(\"_\")[0]  # Assuming filename format: channelname_episode.txt\n",
    "            \n",
    "            with open(os.path.join(transcripts_dir, filename), \"r\") as file:\n",
    "                transcript = file.read()\n",
    "            \n",
    "            summary = summarize_transcript(transcript)\n",
    "            summaries[channel_name].append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "def save_summaries(summaries, output_file):\n",
    "    \"\"\"\n",
    "    Save the grouped summaries to a JSON file\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(summaries, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dir = \"../output/\"\n",
    "output_file = \"fpl_podcast_summaries.json\"\n",
    "\n",
    "summaries = process_transcripts(transcripts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"\n",
    "    Convert non-serializable objects to serializable format.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (str, int, float, bool, type(None))):\n",
    "        return obj\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        # For any other type, convert to string\n",
    "        return str(obj)\n",
    "\n",
    "def save_summaries(summaries, output_file):\n",
    "    \"\"\"\n",
    "    Save the grouped summaries to a JSON file\n",
    "    \"\"\"\n",
    "    serializable_summaries = convert_to_serializable(summaries)\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(serializable_summaries, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Summaries saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to fpl_podcast_summaries.json\n"
     ]
    }
   ],
   "source": [
    "save_summaries(summaries, \"fpl_podcast_summaries.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
