{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load YouTube API key\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the channel ID's from the channel names\n",
    "def get_channel_ids(channel_names):\n",
    "    channel_data = []\n",
    "    \n",
    "    for channel_name in channel_names:\n",
    "        request = youtube.search().list(\n",
    "            q=channel_name,\n",
    "            type='channel',\n",
    "            part='id',\n",
    "            maxResults=1\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        if 'items' in response:\n",
    "            channel_id = response['items'][0]['id']['channelId']\n",
    "            channel_data.append({'channel_name': channel_name, 'channel_id': channel_id})\n",
    "        else:\n",
    "            print(f\"Could not find channel ID for '{channel_name}'\")\n",
    "    \n",
    "    df_channels = pd.DataFrame(channel_data)\n",
    "    return df_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            channel_name                channel_id\n",
      "0          TheArmbandFPL  UC4UdmU9tNnU5iQVmQB3Ngvg\n",
      "1               elitefpl  UCOhHIQyQg4dNKvWg0tg12zg\n",
      "2   fantasyfootballfixYT  UC0Oaf88gRGnNkncI8D_GO-Q\n",
      "3               FFScout_  UCKxYKQ8pgJ7V8wwh4hLsSXQ\n",
      "4        AboveAverageFPL  UCnaJiRMf5hju0TlaeGK5CDQ\n",
      "5              fplbanger  UC1dzUZYYluvh8ktUYFYk8PA\n",
      "6            fplblackbox  UCGJ8-xqhOLwyJNuPMsVoQWQ\n",
      "7               FPLFocal  UC72QokPHXQ9r98ROfNZmaDw\n",
      "8         alwayscheating  UChLRgtHvvYCXWwJFDWmpv8Q\n",
      "9                 FMLFPL  UCZikELJczbLKc_40syGKyxg\n",
      "10           FPLBlackBox  UCGJ8-xqhOLwyJNuPMsVoQWQ\n"
     ]
    }
   ],
   "source": [
    "channel_names = [\n",
    "    \"TheArmbandFPL\",\n",
    "    \"elitefpl\",\n",
    "    \"fantasyfootballfixYT\",\n",
    "    \"FFScout_\",\n",
    "    \"AboveAverageFPL\",\n",
    "    \"fplbanger\",\n",
    "    \"fplblackbox\",\n",
    "    \"FPLFocal\",\n",
    "    \"alwayscheating\",\n",
    "    \"FMLFPL\",\n",
    "    \"FPLBlackBox\"\n",
    "]\n",
    "\n",
    "channel_ids = df_channels = get_channel_ids(channel_names)\n",
    "\n",
    "print(channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the most recent video IDs from a channel\n",
    "def get_channel_videos(df_channels, published_after, max_results=10):\n",
    "    all_videos = []\n",
    "    \n",
    "    for _, row in df_channels.iterrows():\n",
    "        channel_id = row['channel_id']\n",
    "        channel_name = row['channel_name']\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        while True:\n",
    "            request = youtube.search().list(\n",
    "                part=\"id,snippet\",\n",
    "                channelId=channel_id,\n",
    "                type=\"video\",\n",
    "                order=\"date\",\n",
    "                publishedAfter=published_after,\n",
    "                maxResults=max_results,\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                video_id = item['id']['videoId']\n",
    "                title = item['snippet']['title']\n",
    "                published_at = item['snippet']['publishedAt']\n",
    "                videos.append({\n",
    "                    'channel_name': channel_name,\n",
    "                    'id': video_id,\n",
    "                    'title': title,\n",
    "                    'published_at': published_at\n",
    "                })\n",
    "                \n",
    "                if len(videos) >= max_results:\n",
    "                    break\n",
    "            \n",
    "            if len(videos) >= max_results:\n",
    "                break\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "        \n",
    "        all_videos.extend(videos)\n",
    "    \n",
    "    return pd.DataFrame(all_videos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last n video ID's for each of the youtube accounts\n",
    "published_after = \"2024-10-20T00:00:00Z\" \n",
    "df_videos = get_channel_videos(df_channels, published_after, max_results=5)\n",
    "\n",
    "print(df_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grab the transcripts if they exist\n",
    "def get_transcripts(df_videos):\n",
    "    transcripts = []\n",
    "    for _, row in df_videos.iterrows():\n",
    "        video_id = row['id']\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages = ['en'])\n",
    "            full_transcript = ' '.join([entry['text'] for entry in transcript])\n",
    "            transcripts.append(full_transcript)\n",
    "        except Exception as e:\n",
    "            transcripts.append(None)\n",
    "            print(f\"Error getting transcript for video {video_id}: {str(e)}\")\n",
    "    return transcripts\n",
    "\n",
    "# Function to calculate word count\n",
    "def word_count(transcript):\n",
    "    if transcript:\n",
    "        return len(transcript.split())  # Split by whitespace to count words\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting transcript for video Ejwn019lDLM: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=Ejwn019lDLM! This is most likely caused by:\n",
      "\n",
      "Subtitles are disabled for this video\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "Error getting transcript for video jr-Tql57hfk: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=jr-Tql57hfk! This is most likely caused by:\n",
      "\n",
      "Subtitles are disabled for this video\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "            channel_name           id  \\\n",
      "0          TheArmbandFPL  Z5gRIZ2JmT8   \n",
      "1               elitefpl  cDq8wZS55Pg   \n",
      "3               elitefpl  uTnEVxott-0   \n",
      "4   fantasyfootballfixYT  6R7QSFO75Pg   \n",
      "5   fantasyfootballfixYT  IfFcnZ961YE   \n",
      "6   fantasyfootballfixYT  Klh9ZGcZUNw   \n",
      "7               FFScout_  6-Li5N8c7VE   \n",
      "8               FFScout_  KNxOkXemBvE   \n",
      "9               FFScout_  PidLH3k-HAQ   \n",
      "10              FFScout_  B3f9vFGBXYY   \n",
      "11              FFScout_  HlU6h0biZaI   \n",
      "12       AboveAverageFPL  7G7_tv1jLX0   \n",
      "13       AboveAverageFPL  FTE5f0QINDE   \n",
      "14             fplbanger  UQYBt4uIAdA   \n",
      "15           fplblackbox  D8SrQTPj1yE   \n",
      "16           fplblackbox  W7RXnwehNsE   \n",
      "17              FPLFocal  USzbfmj7ID0   \n",
      "19              FPLFocal  9rhIEl1QeqI   \n",
      "22        alwayscheating  PV0vRLn_cRk   \n",
      "25                FMLFPL  ohZqMwGydVQ   \n",
      "26           FPLBlackBox  D8SrQTPj1yE   \n",
      "27           FPLBlackBox  W7RXnwehNsE   \n",
      "\n",
      "                                                title  word_count  \\\n",
      "0        FPL GW5 PREVIEW | BAD TO WORSE | FPL ARMBAND        6416   \n",
      "1   FPL GAMEWEEK 5 PREVIEW | Calvert Lewin Captain...        7714   \n",
      "3   FPL GAMEWEEK 5 SUNDAY SURGERY | Wildcard FAILI...       11967   \n",
      "4   ðŸ¤– AI FPL BEST TEAM REVEAL &amp; Top Captain Pi...        3180   \n",
      "5   â™»ï¸ BEST ISAK REPLACEMENTS | TOP Fantasy Premie...        3558   \n",
      "6   ðŸ”¥ FPL EXPERT TEAM REVEAL | Salah In? | Fantasy...        6425   \n",
      "7   JOAO PEDRO + WATKINS TO START?! ðŸš¨ | FPL TEAM N...        6222   \n",
      "8   FPL DEADLINE DILEMMAS GW5! ðŸ’¥ | FINAL THOUGHTS!...        8932   \n",
      "9   FPL GW5 Q&amp;A! â‰ï¸ | AZ + SAM! | Fantasy Prem...       10134   \n",
      "10  ROBERTSON IN?! ðŸ‘€ | JOE&#39;S FPL GW5 TEAM SELE...        5365   \n",
      "11  TOM F&#39;S GW5 FPL TEAM SELECTION! ðŸ’¥ | SELL M...        6088   \n",
      "12  FPL GW5 Team Selection | Fantasy Premier Leagu...        9106   \n",
      "13  FPL GW4 Reaction - Worst Gameweek Of The Seaso...       17879   \n",
      "14  FPL GW5: PEDRO ISAK OUT AND CAP WATKINS ? - FP...        7354   \n",
      "15  FPL BlackBox | Machine Learning | Fantasy Prem...       27378   \n",
      "16                               FPL Not Enough | GW4        3919   \n",
      "17          FINAL FPL THOUGHTS | GW5 | INJURY NEWS âš ï¸        2092   \n",
      "19                             FPL GW5 EXPERTS TEAM ðŸš€        1810   \n",
      "22  Josh&#39;s Wildcard Passes the Point of No Ret...       16035   \n",
      "25               Ep. 478 - On to GW5 - Tick Every Box       17898   \n",
      "26  FPL BlackBox | Machine Learning | Fantasy Prem...       27378   \n",
      "27                               FPL Not Enough | GW4        3919   \n",
      "\n",
      "                                           transcript  \n",
      "0   sh hello everyone and welcome to the FPL armba...  \n",
      "1   I'm running away from my past I left behind I'...  \n",
      "3   [Music] I'm running away from my past I left b...  \n",
      "4   hello everybody and welcome back to the fantas...  \n",
      "5   yes guys welcome back to another fancy footbal...  \n",
      "6   hi everyone and welcome back to the fantasy fo...  \n",
      "7   [Music] [Music] [Applause] hello and welcome t...  \n",
      "8   [Music] [Applause] hi everyone welcome to dead...  \n",
      "9   [Music] [Applause] good afternoon everyone wel...  \n",
      "10  [Music] [Applause] hello and welcome to anothe...  \n",
      "11  [Music] [Applause] hello and welcome to anothe...  \n",
      "12  [Music] la [Music] right guys welcome back thi...  \n",
      "13  [Music] n [Music] right guys welcome back this...  \n",
      "14  I think I think Walkins is a good differential...  \n",
      "15  [Music] [Music] [Music] [Applause] [Music] [Mu...  \n",
      "16  [Music] [Music] [Music] it's game week four it...  \n",
      "17  welcome back for another video final video of ...  \n",
      "19  welcome back for another video we're back with...  \n",
      "22  and and we are live now Fully live Fully live ...  \n",
      "25  support for this podcast comes from the patron...  \n",
      "26  [Music] [Music] [Music] [Applause] [Music] [Mu...  \n",
      "27  [Music] [Music] [Music] it's game week four it...  \n"
     ]
    }
   ],
   "source": [
    "# Fetch transcripts for df_videos\n",
    "transcripts = get_transcripts(df_videos)\n",
    "\n",
    "# Add transcripts to the DataFrame\n",
    "df_videos['transcript'] = transcripts\n",
    "\n",
    "# Add word count of the transcripts\n",
    "df_videos['word_count'] = df_videos['transcript'].apply(word_count)\n",
    "\n",
    "# Only consider transripts with more than 1000 words\n",
    "df_videos = df_videos[df_videos['word_count'] >= 1000]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_videos[['channel_name', 'id', 'title', 'word_count', 'transcript']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split transcript into manageable chunks based on token count\n",
    "def split_into_chunks(transcript, max_tokens=4000):\n",
    "    # Tokenize the transcript\n",
    "    doc = nlp(transcript)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sentence_tokens = len(sentence.orth_.split())\n",
    "\n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            # If the current chunk exceeds the limit, start a new one\n",
    "            chunks.append(' '.join([str(sent) for sent in current_chunk]))\n",
    "            current_chunk = [sentence.text]\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current_chunk.append(sentence.text)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Append any remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join([str(sent) for sent in current_chunk]))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Preprocess transcripts to reduce token count\n",
    "def preprocess_transcript(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove text in square brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Remove filler words\n",
    "    filler_words = r'\\b(basically|um|umm|uh|oh|yeah|actually|literally|obviously|you know|I mean|I guess|but you know|I suppose|or something|really|very much|sort of|kind of)\\b'\n",
    "    text = re.sub(filler_words, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove repeated words\n",
    "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "    \n",
    "    # Simplify large numbers\n",
    "    text = re.sub(r'\\b(\\d+) thousand\\b', r'\\1k', text)\n",
    "    text = re.sub(r'\\b(\\d+) million\\b', r'\\1m', text)\n",
    "    \n",
    "    # Remove unnecessary punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Use abbreviations for common terms\n",
    "    abbreviations = {\n",
    "        'fantasy premier league': 'fpl',\n",
    "        'gameweek': 'gw',\n",
    "        'manchester united': 'man utd',\n",
    "        'manchester city': 'man city'\n",
    "    }\n",
    "    for full, abbr in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + full + r'\\b', abbr, text)\n",
    "    \n",
    "    # Simplify season references\n",
    "    text = re.sub(r'\\d{4}/\\d{4}\\s+season', 'last season', text)\n",
    "    \n",
    "    # Simplify player names (example for Haaland)\n",
    "    text = re.sub(r'\\berling haaland\\b', 'haaland', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos['transcript_chunks'] = df_videos['transcript'].apply(preprocess_transcript)\n",
    "df_videos['word_count_process'] = df_videos['transcript_chunks'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           channel_name_  word_count_mean  word_count_median  word_count_min  \\\n",
      "1               FFScout_      7348.200000             6222.0            5365   \n",
      "8   fantasyfootballfixYT      4387.666667             3558.0            3180   \n",
      "0        AboveAverageFPL     13492.500000            13492.5            9106   \n",
      "3            FPLBlackBox     15648.500000            15648.5            3919   \n",
      "4               FPLFocal      1951.000000             1951.0            1810   \n",
      "7               elitefpl      9840.500000             9840.5            7714   \n",
      "10           fplblackbox     15648.500000            15648.5            3919   \n",
      "2                 FMLFPL     17898.000000            17898.0           17898   \n",
      "5          TheArmbandFPL      6416.000000             6416.0            6416   \n",
      "6         alwayscheating     16035.000000            16035.0           16035   \n",
      "9              fplbanger      7354.000000             7354.0            7354   \n",
      "\n",
      "    word_count_max  word_count_sum  num_videos  word_count_process_mean  \\\n",
      "1            10134           36741           5              6797.400000   \n",
      "8             6425           13163           3              4152.666667   \n",
      "0            17879           26985           2             12460.500000   \n",
      "3            27378           31297           2             14395.500000   \n",
      "4             2092            3902           2              1935.000000   \n",
      "7            11967           19681           2              9209.500000   \n",
      "10           27378           31297           2             14395.500000   \n",
      "2            17898           17898           1             15765.000000   \n",
      "5             6416            6416           1              5789.000000   \n",
      "6            16035           16035           1             14225.000000   \n",
      "9             7354            7354           1              6786.000000   \n",
      "\n",
      "    word_count_process_median  word_count_process_min  word_count_process_max  \\\n",
      "1                      5683.0                    4879                    9462   \n",
      "8                      3496.0                    3066                    5896   \n",
      "0                     12460.5                    8396                   16525   \n",
      "3                     14395.5                    3695                   25096   \n",
      "4                      1935.0                    1796                    2074   \n",
      "7                      9209.5                    7234                   11185   \n",
      "10                    14395.5                    3695                   25096   \n",
      "2                     15765.0                   15765                   15765   \n",
      "5                      5789.0                    5789                    5789   \n",
      "6                     14225.0                   14225                   14225   \n",
      "9                      6786.0                    6786                    6786   \n",
      "\n",
      "    word_count_process_sum  percent_reduction  \n",
      "1                    33987           7.495713  \n",
      "8                    12458           5.355922  \n",
      "0                    24921           7.648694  \n",
      "3                    28791           8.007157  \n",
      "4                     3870           0.820092  \n",
      "7                    18419           6.412276  \n",
      "10                   28791           8.007157  \n",
      "2                    15765          11.917533  \n",
      "5                     5789           9.772444  \n",
      "6                    14225          11.287808  \n",
      "9                     6786           7.723688  \n"
     ]
    }
   ],
   "source": [
    "# Summary of processing reduction effeciency\n",
    "summary = df_videos.groupby('channel_name').agg({\n",
    "    'word_count': ['mean', 'median', 'min', 'max', 'sum', 'count'],\n",
    "    'word_count_process': ['mean', 'median', 'min', 'max', 'sum']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the column names\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "\n",
    "# Rename the count column to 'num_videos'\n",
    "summary = summary.rename(columns={'word_count_count': 'num_videos'})\n",
    "\n",
    "# Calculate the percentage reduction in word count\n",
    "summary['percent_reduction'] = (1 - summary['word_count_process_sum'] / summary['word_count_sum']) * 100\n",
    "\n",
    "# Sort by number of videos, descending\n",
    "summary = summary.sort_values('num_videos', ascending=False)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos.to_csv(f'../output/transcripts_{published_after}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: TheArmbandFPL_2024-09-19T04:41:29Z.csv\n",
      "Saved: elitefpl_2024-09-19T07:56:44Z.csv\n",
      "Saved: elitefpl_2024-09-16T08:19:03Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-20T09:04:07Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-19T10:00:16Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-18T11:00:21Z.csv\n",
      "Saved: FFScout__2024-09-20T14:41:20Z.csv\n",
      "Saved: FFScout__2024-09-20T08:39:25Z.csv\n",
      "Saved: FFScout__2024-09-19T16:25:37Z.csv\n",
      "Saved: FFScout__2024-09-19T07:05:00Z.csv\n",
      "Saved: FFScout__2024-09-18T17:16:23Z.csv\n",
      "Saved: AboveAverageFPL_2024-09-19T20:31:08Z.csv\n",
      "Saved: AboveAverageFPL_2024-09-15T22:43:31Z.csv\n",
      "Saved: fplbanger_2024-09-19T05:57:48Z.csv\n",
      "Saved: fplblackbox_2024-09-18T20:32:12Z.csv\n",
      "Saved: fplblackbox_2024-09-16T20:07:27Z.csv\n",
      "Saved: FPLFocal_2024-09-20T13:12:47Z.csv\n",
      "Saved: FPLFocal_2024-09-19T11:34:17Z.csv\n",
      "Saved: alwayscheating_2024-09-16T13:28:55Z.csv\n",
      "Saved: FMLFPL_2024-09-17T07:20:47Z.csv\n",
      "Saved: FPLBlackBox_2024-09-18T20:32:12Z.csv\n",
      "Saved: FPLBlackBox_2024-09-16T20:07:27Z.csv\n"
     ]
    }
   ],
   "source": [
    "def save_transcripts_as_csv(df, output_dir):\n",
    "    \"\"\"\n",
    "    Save transcripts from DataFrame as individual CSV files\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the transcripts\n",
    "    output_dir (str): Directory to save the CSV files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the DataFrame has the necessary columns\n",
    "    required_columns = ['channel_name', 'published_at', 'transcript_chunks']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame is missing one or more required columns: {required_columns}\")\n",
    "    \n",
    "    # Iterate through the DataFrame and save each transcript as a CSV file\n",
    "    for _, row in df.iterrows():\n",
    "        channel_name = row['channel_name']\n",
    "        episode_number = row['published_at']\n",
    "        transcript = row['transcript_chunks']\n",
    "        \n",
    "        # Create the filename\n",
    "        filename = f\"{channel_name}_{episode_number}.csv\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Create a new DataFrame with just the transcript\n",
    "        transcript_df = pd.DataFrame({'transcript': [transcript]})\n",
    "        \n",
    "        # Save the transcript as a CSV file\n",
    "        transcript_df.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "output_directory = \"../output/transcripts/\"\n",
    "save_transcripts_as_csv(df_videos, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "def summarize_with_bart(text, max_length=150, min_length=50):\n",
    "    # Check if MPS is available and set the device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        # Load pre-trained model and tokenizer\n",
    "        model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "        tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
    "        \n",
    "        # Move input tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate summary\n",
    "        summary_ids = model.generate(inputs['input_ids'],\n",
    "                                     num_beams=4,\n",
    "                                     max_length=max_length,\n",
    "                                     min_length=min_length,\n",
    "                                     length_penalty=2.0,\n",
    "                                     early_stopping=True)\n",
    "\n",
    "        # Decode the generated summary\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during summarization: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# disabled for now, summarising with Claude 3.5\n",
    "#summary = summarize_with_bart(df['transcript_chunks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "anthropic = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def summarize_transcript(transcript):\n",
    "    \"\"\"\n",
    "    Summarize a single transcript using Claude API\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Please summarize the following Fantasy Premier League podcast transcript. \n",
    "    Focus on the key points, player recommendations, and strategy advice.\n",
    "    Limit the summary to 3-5 bullet points.\n",
    "\n",
    "    Transcript:\n",
    "    {transcript}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    response = anthropic.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.5,\n",
    "        system=\"You are an expert in Fantasy Premier League and podcast summarization.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.content\n",
    "\n",
    "def process_transcripts(transcripts_dir):\n",
    "    \"\"\"\n",
    "    Process all transcripts in the given directory and group summaries by channel name\n",
    "    \"\"\"\n",
    "    summaries = defaultdict(list)\n",
    "\n",
    "    for filename in os.listdir(transcripts_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            channel_name = filename.split(\"_\")[0]\n",
    "            \n",
    "            with open(os.path.join(transcripts_dir, filename), \"r\") as file:\n",
    "                transcript = file.read()\n",
    "            \n",
    "            summary = summarize_transcript(transcript)\n",
    "            summaries[channel_name].append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "def save_summaries(summaries, output_file):\n",
    "    \"\"\"\n",
    "    Save the grouped summaries to a JSON file\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(summaries, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dir = \"../output/transcripts/\"\n",
    "output_file = \"fpl_podcast_summaries.json\"\n",
    "\n",
    "summaries = process_transcripts(transcripts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"\n",
    "    Convert non-serializable objects to serializable format.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (str, int, float, bool, type(None))):\n",
    "        return obj\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        # For any other type, convert to string\n",
    "        return str(obj)\n",
    "\n",
    "def save_summaries(summaries, output_file):\n",
    "    \"\"\"\n",
    "    Save the grouped summaries to a JSON file\n",
    "    \"\"\"\n",
    "    serializable_summaries = convert_to_serializable(summaries)\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(serializable_summaries, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Summaries saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to ../output/fpl_podcast_summaries.json\n"
     ]
    }
   ],
   "source": [
    "save_summaries(summaries, \"../output/fpl_podcast_summaries.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove TextBlock wrapper and unescape newlines\n",
    "    text = re.sub(r'TextBlock\\(text=|,\\s*type=\\'text\\'\\)', '', text)\n",
    "    text = text.strip(\"'\")\n",
    "    # Replace escaped newlines with actual newlines\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    # Remove the introductory sentence\n",
    "    pattern = r\"Here's a summary of the key points from the Fantasy Premier League podcast transcript:[\\s\\n]*\"\n",
    "    text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return text.strip()\n",
    "\n",
    "def is_summarizable(text):\n",
    "    unsummarizable_patterns = [\n",
    "        \"I apologize, but I cannot provide a meaningful summary\",\n",
    "        \"I'm sorry, but I can't provide a summary\",\n",
    "        \"Unable to provide a summary\"\n",
    "    ]\n",
    "    cleaned_text = clean_text(text).lower()\n",
    "    return not any(pattern.lower() in cleaned_text for pattern in unsummarizable_patterns)\n",
    "\n",
    "def filter_json(json_data):\n",
    "    filtered_data = {}\n",
    "    for channel, summaries in json_data.items():\n",
    "        filtered_summaries = [summary for summary in summaries if is_summarizable(summary[0])]\n",
    "        if filtered_summaries:\n",
    "            filtered_data[channel] = filtered_summaries\n",
    "    return filtered_data\n",
    "\n",
    "def json_to_markdown(json_data):\n",
    "    markdown = \"\"\n",
    "    \n",
    "    for channel, summaries in json_data.items():\n",
    "        markdown += f\"# {channel}\\n\\n\"\n",
    "        \n",
    "        for summary in summaries:\n",
    "            cleaned_summary = clean_text(summary[0])\n",
    "            # Add two spaces at the end of each line for Markdown line breaks\n",
    "            cleaned_summary = '\\n'.join(line.rstrip() + '  ' for line in cleaned_summary.split('\\n'))\n",
    "            markdown += cleaned_summary + \"\\n\\n\"\n",
    "        \n",
    "        markdown += \"---\\n\\n\"\n",
    "    \n",
    "    return markdown.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Markdown file 'fpl_podcast_summaries.md' has been created.\n",
      "Filtered JSON file 'filtered_fpl_podcast_summaries.json' has been created.\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open('../output/fpl_podcast_summaries.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter out unsummarizable transcripts\n",
    "filtered_data = filter_json(data)\n",
    "\n",
    "# Convert filtered JSON to Markdown\n",
    "markdown_content = json_to_markdown(filtered_data)\n",
    "\n",
    "# Write the Markdown content to a file\n",
    "with open('../output/fpl_podcast_summaries.md', 'w', encoding='utf-8') as file:\n",
    "    file.write(markdown_content)\n",
    "\n",
    "print(\"Conversion complete. Markdown file 'fpl_podcast_summaries.md' has been created.\")\n",
    "\n",
    "# Optionally, save the filtered JSON\n",
    "with open('../output/filtered_fpl_podcast_summaries.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(filtered_data, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Filtered JSON file 'filtered_fpl_podcast_summaries.json' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_word_frequencies(text, min_length=1):\n",
    "    # Convert to lowercase and split into words\n",
    "    words = re.findall(r'\\b[\\w\\']+\\b', text.lower())\n",
    "    \n",
    "    # Filter words by minimum length if specified\n",
    "    if min_length > 1:\n",
    "        words = [word for word in words if len(word) >= min_length]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "# Combine all transcripts into a single string\n",
    "all_transcripts = ' '.join(df_videos['transcript'])\n",
    "\n",
    "# Get word frequencies\n",
    "word_frequencies = get_word_frequencies(all_transcripts)\n",
    "\n",
    "# Get the 100 most common words\n",
    "most_common_words = word_frequencies.most_common(100)\n",
    "\n",
    "# Print the results\n",
    "print(\"100 Most Common Words:\")\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
