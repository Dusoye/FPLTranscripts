{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load YouTube API key\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the channel ID's from the channel names\n",
    "def get_channel_ids(channel_names):\n",
    "    channel_data = []\n",
    "    \n",
    "    for channel_name in channel_names:\n",
    "        request = youtube.search().list(\n",
    "            q=channel_name,\n",
    "            type='channel',\n",
    "            part='id',\n",
    "            maxResults=1\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        if 'items' in response:\n",
    "            channel_id = response['items'][0]['id']['channelId']\n",
    "            channel_data.append({'channel_name': channel_name, 'channel_id': channel_id})\n",
    "        else:\n",
    "            print(f\"Could not find channel ID for '{channel_name}'\")\n",
    "    \n",
    "    df_channels = pd.DataFrame(channel_data)\n",
    "    return df_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            channel_name                channel_id\n",
      "0          TheArmbandFPL  UC4UdmU9tNnU5iQVmQB3Ngvg\n",
      "1               elitefpl  UCOhHIQyQg4dNKvWg0tg12zg\n",
      "2   fantasyfootballfixYT  UC0Oaf88gRGnNkncI8D_GO-Q\n",
      "3               FFScout_  UCKxYKQ8pgJ7V8wwh4hLsSXQ\n",
      "4        AboveAverageFPL  UCnaJiRMf5hju0TlaeGK5CDQ\n",
      "5              fplbanger  UC1dzUZYYluvh8ktUYFYk8PA\n",
      "6            fplblackbox  UCGJ8-xqhOLwyJNuPMsVoQWQ\n",
      "7               FPLFocal  UC72QokPHXQ9r98ROfNZmaDw\n",
      "8         alwayscheating  UChLRgtHvvYCXWwJFDWmpv8Q\n",
      "9                 FMLFPL  UCZikELJczbLKc_40syGKyxg\n",
      "10           FPLBlackBox  UCGJ8-xqhOLwyJNuPMsVoQWQ\n"
     ]
    }
   ],
   "source": [
    "channel_names = [\n",
    "    \"TheArmbandFPL\",\n",
    "    \"elitefpl\",\n",
    "    \"fantasyfootballfixYT\",\n",
    "    \"FFScout_\",\n",
    "    \"AboveAverageFPL\",\n",
    "    \"fplbanger\",\n",
    "    \"fplblackbox\",\n",
    "    \"FPLFocal\",\n",
    "    \"alwayscheating\",\n",
    "    \"FMLFPL\",\n",
    "    \"FPLBlackBox\"\n",
    "]\n",
    "\n",
    "channel_ids = df_channels = get_channel_ids(channel_names)\n",
    "\n",
    "print(channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the most recent video IDs from a channel\n",
    "def get_channel_videos(df_channels, published_after, max_results=10):\n",
    "    all_videos = []\n",
    "    \n",
    "    for _, row in df_channels.iterrows():\n",
    "        channel_id = row['channel_id']\n",
    "        channel_name = row['channel_name']\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        while True:\n",
    "            request = youtube.search().list(\n",
    "                part=\"id,snippet\",\n",
    "                channelId=channel_id,\n",
    "                type=\"video\",\n",
    "                order=\"date\",\n",
    "                publishedAfter=published_after,\n",
    "                maxResults=max_results,\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response['items']:\n",
    "                video_id = item['id']['videoId']\n",
    "                title = item['snippet']['title']\n",
    "                published_at = item['snippet']['publishedAt']\n",
    "                videos.append({\n",
    "                    'channel_name': channel_name,\n",
    "                    'id': video_id,\n",
    "                    'title': title,\n",
    "                    'published_at': published_at\n",
    "                })\n",
    "                \n",
    "                if len(videos) >= max_results:\n",
    "                    break\n",
    "            \n",
    "            if len(videos) >= max_results:\n",
    "                break\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "        \n",
    "        all_videos.extend(videos)\n",
    "    \n",
    "    return pd.DataFrame(all_videos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            channel_name           id  \\\n",
      "0          TheArmbandFPL  kqgIuTxdrdo   \n",
      "1          TheArmbandFPL  GUVhMwx6oMI   \n",
      "2               elitefpl  pbpK3bPss9Q   \n",
      "3               elitefpl  r3VkzwaZnSE   \n",
      "4               elitefpl  tZOBHSM-g5Q   \n",
      "5   fantasyfootballfixYT  dOewUlqqdAk   \n",
      "6   fantasyfootballfixYT  OyxDarrEgJ8   \n",
      "7   fantasyfootballfixYT  JqYYXLEJpd4   \n",
      "8   fantasyfootballfixYT  JcNY6U3xW7A   \n",
      "9   fantasyfootballfixYT  4nszN8HLqtI   \n",
      "10              FFScout_  jdSTS01bMSs   \n",
      "11              FFScout_  Lr9B4aNvLn0   \n",
      "12              FFScout_  jfq0eHtXsOQ   \n",
      "13              FFScout_  mq6v-RP0IlY   \n",
      "14              FFScout_  16_aM49v5Y4   \n",
      "15       AboveAverageFPL  vqDCHu7ctdE   \n",
      "16       AboveAverageFPL  Z0ZTwV5hkUE   \n",
      "17       AboveAverageFPL  UZmioRRUiBc   \n",
      "18       AboveAverageFPL  mEm14WbHwXc   \n",
      "19       AboveAverageFPL  rVXFYd-BB98   \n",
      "20             fplbanger  3duJT4qFGsk   \n",
      "21           fplblackbox  Ymiink3oVcQ   \n",
      "22           fplblackbox  l0oClUyq9nk   \n",
      "23              FPLFocal  wHtcEjFuzuw   \n",
      "24              FPLFocal  RWNXvIig1qY   \n",
      "25              FPLFocal  zD10l1-B4iU   \n",
      "26              FPLFocal  0wfY1Oy-3RM   \n",
      "27              FPLFocal  piehsT9cyO8   \n",
      "28        alwayscheating  -GjFsWPxH94   \n",
      "29        alwayscheating  fN024gTrgZ0   \n",
      "30        alwayscheating  5Lkrt-h1r1M   \n",
      "31        alwayscheating  aeJrbuv3Nb8   \n",
      "32                FMLFPL  5Y9jYDwdmV4   \n",
      "33                FMLFPL  HX0FsXB_6Vo   \n",
      "34                FMLFPL  AdoZdd_cMQk   \n",
      "35                FMLFPL  y0qxE_gkhbg   \n",
      "36           FPLBlackBox  Ymiink3oVcQ   \n",
      "37           FPLBlackBox  l0oClUyq9nk   \n",
      "\n",
      "                                                title          published_at  \n",
      "0            FPL GW4 PREVIEW | WC PICKS | FPL ARMBAND  2024-09-12T05:30:55Z  \n",
      "1            FPL GW4 PREVIEW ÔΩú WC PICKS ÔΩú FPL ARMBAND  2024-09-11T18:29:52Z  \n",
      "2   FPL GAMEWEEK 4 PREVIEW | OUR WILDCARDS ARE ACT...  2024-09-12T06:59:47Z  \n",
      "3   FPL GAMEWEEK 4 WILDCARD DRAFT | THE BIG DEBATE...  2024-09-05T21:10:41Z  \n",
      "4   FPL GAMEWEEK 4 SUNDAY SURGERY | LET&#39;S RE-E...  2024-09-01T20:40:54Z  \n",
      "5   ü§ñ AI FPL BEST TEAM REVEAL &amp; Top Captain Pi...  2024-09-13T09:33:25Z  \n",
      "6   üîí FPL GW4 WILDCARD ESSENTIALS | TOP Fantasy Pr...  2024-09-12T09:45:01Z  \n",
      "7   üî• FPL TEAM REVEAL | WILDCARD Draft | Fantasy P...  2024-09-11T10:49:37Z  \n",
      "8   ü§ñ AI WILDCARD FPL TEAM REVEAL GAMEWEEK 4 | Edd...  2024-09-06T12:00:31Z  \n",
      "9   üî• GW4 WILDCARD TEAM: Salah, Haaland &amp; Top ...  2024-09-05T10:00:04Z  \n",
      "10  WILL WATKINS &amp; HAALAND START?! üö® | FPL TEA...  2024-09-13T13:40:31Z  \n",
      "11              BEST #FPL CAPTAIN GAMEWEEK 4! #shorts  2024-09-13T12:30:30Z  \n",
      "12  FPL DEADLINE DILEMMAS GW4! üí• | FINAL THOUGHTS!...  2024-09-13T09:53:21Z  \n",
      "13  FPL GW4 Q&amp;A! ‚ÅâÔ∏è | TOM + SAM! | Fantasy Pre...  2024-09-12T16:13:02Z  \n",
      "14     ‚úÖ #FPL Gameweek 4 BEST PLAYERS TO BUY! #shorts  2024-09-12T12:00:01Z  \n",
      "15  FPL GW4 Wildcard Team Selection | Fantasy Prem...  2024-09-12T21:17:58Z  \n",
      "16          Is Haaland the Greatest Goal Scorer Ever?  2024-09-09T12:04:37Z  \n",
      "17  Mastering Banking Transfers: The Wildcard Adva...  2024-09-09T09:26:32Z  \n",
      "18  Haaland to break all records? Liverpool for th...  2024-09-08T20:57:20Z  \n",
      "19  FPL WILDCARD ACTIVE w/ @LetsTalkFPL | Fantasy ...  2024-09-04T18:27:04Z  \n",
      "20  FPL GW4: 98 PTS &amp; WILDCARD ACTIVE ! -  FPL...  2024-09-10T09:54:46Z  \n",
      "21  FPL BlackBox | Guns Out | Fantasy Premier Leag...  2024-09-12T23:01:54Z  \n",
      "22                                FPL Wildcard? | GW4  2024-09-08T21:18:52Z  \n",
      "23                  FPL Focal FREE app - How to Guide  2024-09-13T15:31:00Z  \n",
      "24           FINAL FPL THOUGHTS GW4 | HAALAND NEWS ‚ö†Ô∏è  2024-09-13T12:23:02Z  \n",
      "25                             FPL GW4 EXPERTS TEAM üöÄ  2024-09-13T07:01:00Z  \n",
      "26          FPL GW4 EXPERTS TEAM | WILDCARD ACTIVE! üöÄ  2024-09-12T10:49:50Z  \n",
      "27                       MY FPL GW4 TEAM | Gameweek 4  2024-09-12T07:01:00Z  \n",
      "28  Josh&#39;s Wildcard Update &amp; Our GW4 FPL P...  2024-09-09T13:32:22Z  \n",
      "29  Josh&#39;s Wildcard Update &amp; Our GW4 FPL P...  2024-09-09T03:05:50Z  \n",
      "30  Common Sense FPL Takes for All 20 Premier Leag...  2024-09-03T12:45:00Z  \n",
      "31  Common Sense FPL Takes for All 20 Premier Leag...  2024-09-03T02:26:53Z  \n",
      "32  Friday My Life - Put The Word Out There [Patre...  2024-09-13T06:47:19Z  \n",
      "33  Alon x LR over at @TheFPLWireFantasyPremierLeague  2024-09-10T20:46:09Z  \n",
      "34                  We Hate International Breaks #fpl  2024-09-03T06:34:11Z  \n",
      "35  Ep. 478 - On to GW4 Intl. Break - Improve Your...  2024-09-03T05:57:58Z  \n",
      "36  FPL BlackBox | Guns Out | Fantasy Premier Leag...  2024-09-12T23:01:54Z  \n",
      "37                                FPL Wildcard? | GW4  2024-09-08T21:18:52Z  \n"
     ]
    }
   ],
   "source": [
    "# Extract the last n video ID's for each of the youtube accounts\n",
    "published_after = \"2024-09-01T00:00:00Z\" \n",
    "df_videos = get_channel_videos(df_channels, published_after, max_results=5)\n",
    "\n",
    "print(df_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grab the transcripts if they exist\n",
    "def get_transcripts(df_videos):\n",
    "    transcripts = []\n",
    "    for _, row in df_videos.iterrows():\n",
    "        video_id = row['id']\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages = ['en'])\n",
    "            full_transcript = ' '.join([entry['text'] for entry in transcript])\n",
    "            transcripts.append(full_transcript)\n",
    "        except Exception as e:\n",
    "            transcripts.append(None)\n",
    "            print(f\"Error getting transcript for video {video_id}: {str(e)}\")\n",
    "    return transcripts\n",
    "\n",
    "# Function to calculate word count\n",
    "def word_count(transcript):\n",
    "    if transcript:\n",
    "        return len(transcript.split())  # Split by whitespace to count words\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting transcript for video jdSTS01bMSs: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=jdSTS01bMSs! This is most likely caused by:\n",
      "\n",
      "Subtitles are disabled for this video\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "Error getting transcript for video mEm14WbHwXc: no element found: line 1, column 0\n",
      "            channel_name           id  \\\n",
      "0          TheArmbandFPL  kqgIuTxdrdo   \n",
      "1          TheArmbandFPL  GUVhMwx6oMI   \n",
      "2               elitefpl  pbpK3bPss9Q   \n",
      "3               elitefpl  r3VkzwaZnSE   \n",
      "4               elitefpl  tZOBHSM-g5Q   \n",
      "5   fantasyfootballfixYT  dOewUlqqdAk   \n",
      "6   fantasyfootballfixYT  OyxDarrEgJ8   \n",
      "7   fantasyfootballfixYT  JqYYXLEJpd4   \n",
      "8   fantasyfootballfixYT  JcNY6U3xW7A   \n",
      "9   fantasyfootballfixYT  4nszN8HLqtI   \n",
      "12              FFScout_  jfq0eHtXsOQ   \n",
      "13              FFScout_  mq6v-RP0IlY   \n",
      "15       AboveAverageFPL  vqDCHu7ctdE   \n",
      "19       AboveAverageFPL  rVXFYd-BB98   \n",
      "20             fplbanger  3duJT4qFGsk   \n",
      "21           fplblackbox  Ymiink3oVcQ   \n",
      "22           fplblackbox  l0oClUyq9nk   \n",
      "24              FPLFocal  RWNXvIig1qY   \n",
      "26              FPLFocal  0wfY1Oy-3RM   \n",
      "28        alwayscheating  -GjFsWPxH94   \n",
      "29        alwayscheating  fN024gTrgZ0   \n",
      "30        alwayscheating  5Lkrt-h1r1M   \n",
      "31        alwayscheating  aeJrbuv3Nb8   \n",
      "35                FMLFPL  y0qxE_gkhbg   \n",
      "36           FPLBlackBox  Ymiink3oVcQ   \n",
      "37           FPLBlackBox  l0oClUyq9nk   \n",
      "\n",
      "                                                title  word_count  \\\n",
      "0            FPL GW4 PREVIEW | WC PICKS | FPL ARMBAND       13294   \n",
      "1            FPL GW4 PREVIEW ÔΩú WC PICKS ÔΩú FPL ARMBAND       13160   \n",
      "2   FPL GAMEWEEK 4 PREVIEW | OUR WILDCARDS ARE ACT...       12072   \n",
      "3   FPL GAMEWEEK 4 WILDCARD DRAFT | THE BIG DEBATE...        9695   \n",
      "4   FPL GAMEWEEK 4 SUNDAY SURGERY | LET&#39;S RE-E...        9184   \n",
      "5   ü§ñ AI FPL BEST TEAM REVEAL &amp; Top Captain Pi...        2831   \n",
      "6   üîí FPL GW4 WILDCARD ESSENTIALS | TOP Fantasy Pr...        3025   \n",
      "7   üî• FPL TEAM REVEAL | WILDCARD Draft | Fantasy P...        7224   \n",
      "8   ü§ñ AI WILDCARD FPL TEAM REVEAL GAMEWEEK 4 | Edd...        3553   \n",
      "9   üî• GW4 WILDCARD TEAM: Salah, Haaland &amp; Top ...        2772   \n",
      "12  FPL DEADLINE DILEMMAS GW4! üí• | FINAL THOUGHTS!...        9890   \n",
      "13  FPL GW4 Q&amp;A! ‚ÅâÔ∏è | TOM + SAM! | Fantasy Pre...       10001   \n",
      "15  FPL GW4 Wildcard Team Selection | Fantasy Prem...       12730   \n",
      "19  FPL WILDCARD ACTIVE w/ @LetsTalkFPL | Fantasy ...       16072   \n",
      "20  FPL GW4: 98 PTS &amp; WILDCARD ACTIVE ! -  FPL...       10304   \n",
      "21  FPL BlackBox | Guns Out | Fantasy Premier Leag...       27897   \n",
      "22                                FPL Wildcard? | GW4        2607   \n",
      "24           FINAL FPL THOUGHTS GW4 | HAALAND NEWS ‚ö†Ô∏è        1990   \n",
      "26          FPL GW4 EXPERTS TEAM | WILDCARD ACTIVE! üöÄ        2428   \n",
      "28  Josh&#39;s Wildcard Update &amp; Our GW4 FPL P...       13413   \n",
      "29  Josh&#39;s Wildcard Update &amp; Our GW4 FPL P...       13640   \n",
      "30  Common Sense FPL Takes for All 20 Premier Leag...       18322   \n",
      "31  Common Sense FPL Takes for All 20 Premier Leag...       18387   \n",
      "35  Ep. 478 - On to GW4 Intl. Break - Improve Your...       18514   \n",
      "36  FPL BlackBox | Guns Out | Fantasy Premier Leag...       27897   \n",
      "37                                FPL Wildcard? | GW4        2607   \n",
      "\n",
      "                                           transcript  \n",
      "0   [Music] [Music] [Music] hello all right how ar...  \n",
      "1   against Leicester so I was just like that's wh...  \n",
      "2   inside and out of my heart just for tonight co...  \n",
      "3   a impromptu I guess live stream uh discussing ...  \n",
      "4   all right let's go I'm running away from the p...  \n",
      "5   hello everybody and welcome back to the fantas...  \n",
      "6   yes guys welcome back to another fanasy footba...  \n",
      "7   hi everyone and welcome back to the fantasy fo...  \n",
      "8   hello everybody and welcome back to the fantas...  \n",
      "9   yes guys welcome back to another fantasy footb...  \n",
      "12  [Music] [Applause] Hello everybody welcome bac...  \n",
      "13  [Applause] good afternoon what is going on eve...  \n",
      "15  [Music] n [Music] right guys welcome back this...  \n",
      "19  three game weeks in and we're now into the fir...  \n",
      "20  so it's an anomaly I feel it will even out the...  \n",
      "21  [Music] [Music] [Music] [Music] [Music] [Music...  \n",
      "22  [Music] [Music] right game week three it's uh ...  \n",
      "24  welcome back for another video final video bef...  \n",
      "26  welcome back for another video time to get bac...  \n",
      "28  all right behind the curtain before we uh befo...  \n",
      "29  hi and welcome to Ikea how can I help oh my sc...  \n",
      "30  it's like it's fine all right we are live sir ...  \n",
      "31  [Music] [Applause] hail cheaters welcome to th...  \n",
      "35  support for this podcast comes from the patron...  \n",
      "36  [Music] [Music] [Music] [Music] [Music] [Music...  \n",
      "37  [Music] [Music] right game week three it's uh ...  \n"
     ]
    }
   ],
   "source": [
    "# Fetch transcripts for df_videos\n",
    "transcripts = get_transcripts(df_videos)\n",
    "\n",
    "# Add transcripts to the DataFrame\n",
    "df_videos['transcript'] = transcripts\n",
    "\n",
    "# Add word count of the transcripts\n",
    "df_videos['word_count'] = df_videos['transcript'].apply(word_count)\n",
    "\n",
    "# Only consider transripts with more than 1000 words\n",
    "df_videos = df_videos[df_videos['word_count'] >= 1000]\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_videos[['channel_name', 'id', 'title', 'word_count', 'transcript']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split transcript into manageable chunks based on token count\n",
    "def split_into_chunks(transcript, max_tokens=4000):\n",
    "    # Tokenize the transcript\n",
    "    doc = nlp(transcript)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sentence_tokens = len(sentence.orth_.split())\n",
    "\n",
    "        if current_tokens + sentence_tokens > max_tokens:\n",
    "            # If the current chunk exceeds the limit, start a new one\n",
    "            chunks.append(' '.join([str(sent) for sent in current_chunk]))\n",
    "            current_chunk = [sentence.text]\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current_chunk.append(sentence.text)\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    # Append any remaining chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join([str(sent) for sent in current_chunk]))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Preprocess transcripts to reduce token count\n",
    "def preprocess_transcript(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove text in square brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Remove filler words\n",
    "    filler_words = r'\\b(basically|um|umm|uh|oh|yeah|actually|literally|obviously|you know|I mean|I guess|but you know|I suppose|or something|really|very much|sort of|kind of)\\b'\n",
    "    text = re.sub(filler_words, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove repeated words\n",
    "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "    \n",
    "    # Simplify large numbers\n",
    "    text = re.sub(r'\\b(\\d+) thousand\\b', r'\\1k', text)\n",
    "    text = re.sub(r'\\b(\\d+) million\\b', r'\\1m', text)\n",
    "    \n",
    "    # Remove unnecessary punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Use abbreviations for common terms\n",
    "    abbreviations = {\n",
    "        'fantasy premier league': 'fpl',\n",
    "        'gameweek': 'gw',\n",
    "        'manchester united': 'man utd',\n",
    "        'manchester city': 'man city'\n",
    "    }\n",
    "    for full, abbr in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + full + r'\\b', abbr, text)\n",
    "    \n",
    "    # Simplify season references\n",
    "    text = re.sub(r'\\d{4}/\\d{4}\\s+season', 'last season', text)\n",
    "    \n",
    "    # Simplify player names (example for Haaland)\n",
    "    text = re.sub(r'\\berling haaland\\b', 'haaland', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/kjxldrfs36qgygzmg25sqy100000gn/T/ipykernel_58762/3286257468.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_videos['transcript_chunks'] = df_videos['transcript'].apply(preprocess_transcript)\n",
      "/var/folders/ss/kjxldrfs36qgygzmg25sqy100000gn/T/ipykernel_58762/3286257468.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_videos['word_count_process'] = df_videos['transcript_chunks'].apply(word_count)\n"
     ]
    }
   ],
   "source": [
    "df_videos['transcript_chunks'] = df_videos['transcript'].apply(preprocess_transcript)\n",
    "df_videos['word_count_process'] = df_videos['transcript_chunks'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           channel_name_  word_count_mean  word_count_median  word_count_min  \\\n",
      "8   fantasyfootballfixYT           3881.0             3025.0            2772   \n",
      "6         alwayscheating          15940.5            15981.0           13413   \n",
      "7               elitefpl          10317.0             9695.0            9184   \n",
      "0        AboveAverageFPL          14401.0            14401.0           12730   \n",
      "1               FFScout_           9945.5             9945.5            9890   \n",
      "3            FPLBlackBox          15252.0            15252.0            2607   \n",
      "4               FPLFocal           2209.0             2209.0            1990   \n",
      "5          TheArmbandFPL          13227.0            13227.0           13160   \n",
      "10           fplblackbox          15252.0            15252.0            2607   \n",
      "2                 FMLFPL          18514.0            18514.0           18514   \n",
      "9              fplbanger          10304.0            10304.0           10304   \n",
      "\n",
      "    word_count_max  word_count_sum  num_videos  word_count_process_mean  \\\n",
      "8             7224           19405           5                  3714.60   \n",
      "6            18387           63762           4                 14316.75   \n",
      "7            12072           30951           3                  9773.00   \n",
      "0            16072           28802           2                 13386.50   \n",
      "1            10001           19891           2                  9368.50   \n",
      "3            27897           30504           2                 14131.00   \n",
      "4             2428            4418           2                  2194.50   \n",
      "5            13294           26454           2                 12033.00   \n",
      "10           27897           30504           2                 14131.00   \n",
      "2            18514           18514           1                 16396.00   \n",
      "9            10304           10304           1                  9557.00   \n",
      "\n",
      "    word_count_process_median  word_count_process_min  word_count_process_max  \\\n",
      "8                      2987.0                    2692                    6702   \n",
      "6                     14371.5                   12041                   16483   \n",
      "7                      9229.0                    8658                   11432   \n",
      "0                     13386.5                   11676                   15097   \n",
      "1                      9368.5                    9291                    9446   \n",
      "3                     14131.0                    2474                   25788   \n",
      "4                      2194.5                    1975                    2414   \n",
      "5                     12033.0                   11976                   12090   \n",
      "10                    14131.0                    2474                   25788   \n",
      "2                     16396.0                   16396                   16396   \n",
      "9                      9557.0                    9557                    9557   \n",
      "\n",
      "    word_count_process_sum  percent_reduction  \n",
      "8                    18573           4.287555  \n",
      "6                    57267          10.186318  \n",
      "7                    29319           5.272851  \n",
      "0                    26773           7.044650  \n",
      "1                    18737           5.801619  \n",
      "3                    28262           7.349856  \n",
      "4                     4389           0.656406  \n",
      "5                    24066           9.026990  \n",
      "10                   28262           7.349856  \n",
      "2                    16396          11.439991  \n",
      "9                     9557           7.249612  \n"
     ]
    }
   ],
   "source": [
    "# Summary of processing reduction effeciency\n",
    "summary = df_videos.groupby('channel_name').agg({\n",
    "    'word_count': ['mean', 'median', 'min', 'max', 'sum', 'count'],\n",
    "    'word_count_process': ['mean', 'median', 'min', 'max', 'sum']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the column names\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "\n",
    "# Rename the count column to 'num_videos'\n",
    "summary = summary.rename(columns={'word_count_count': 'num_videos'})\n",
    "\n",
    "# Calculate the percentage reduction in word count\n",
    "summary['percent_reduction'] = (1 - summary['word_count_process_sum'] / summary['word_count_sum']) * 100\n",
    "\n",
    "# Sort by number of videos, descending\n",
    "summary = summary.sort_values('num_videos', ascending=False)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos.to_csv(f'../output/transcripts_{published_after}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: TheArmbandFPL_2024-09-12T05:30:55Z.csv\n",
      "Saved: TheArmbandFPL_2024-09-11T18:29:52Z.csv\n",
      "Saved: elitefpl_2024-09-12T06:59:47Z.csv\n",
      "Saved: elitefpl_2024-09-05T21:10:41Z.csv\n",
      "Saved: elitefpl_2024-09-01T20:40:54Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-13T09:33:25Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-12T09:45:01Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-11T10:49:37Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-06T12:00:31Z.csv\n",
      "Saved: fantasyfootballfixYT_2024-09-05T10:00:04Z.csv\n",
      "Saved: FFScout__2024-09-13T09:53:21Z.csv\n",
      "Saved: FFScout__2024-09-12T16:13:02Z.csv\n",
      "Saved: AboveAverageFPL_2024-09-12T21:17:58Z.csv\n",
      "Saved: AboveAverageFPL_2024-09-04T18:27:04Z.csv\n",
      "Saved: fplbanger_2024-09-10T09:54:46Z.csv\n",
      "Saved: fplblackbox_2024-09-12T23:01:54Z.csv\n",
      "Saved: fplblackbox_2024-09-08T21:18:52Z.csv\n",
      "Saved: FPLFocal_2024-09-13T12:23:02Z.csv\n",
      "Saved: FPLFocal_2024-09-12T10:49:50Z.csv\n",
      "Saved: alwayscheating_2024-09-09T13:32:22Z.csv\n",
      "Saved: alwayscheating_2024-09-09T03:05:50Z.csv\n",
      "Saved: alwayscheating_2024-09-03T12:45:00Z.csv\n",
      "Saved: alwayscheating_2024-09-03T02:26:53Z.csv\n",
      "Saved: FMLFPL_2024-09-03T05:57:58Z.csv\n",
      "Saved: FPLBlackBox_2024-09-12T23:01:54Z.csv\n",
      "Saved: FPLBlackBox_2024-09-08T21:18:52Z.csv\n"
     ]
    }
   ],
   "source": [
    "def save_transcripts_as_csv(df, output_dir):\n",
    "    \"\"\"\n",
    "    Save transcripts from DataFrame as individual CSV files\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the transcripts\n",
    "    output_dir (str): Directory to save the CSV files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the DataFrame has the necessary columns\n",
    "    required_columns = ['channel_name', 'published_at', 'transcript_chunks']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise ValueError(f\"DataFrame is missing one or more required columns: {required_columns}\")\n",
    "    \n",
    "    # Iterate through the DataFrame and save each transcript as a CSV file\n",
    "    for _, row in df.iterrows():\n",
    "        channel_name = row['channel_name']\n",
    "        episode_number = row['published_at']\n",
    "        transcript = row['transcript_chunks']\n",
    "        \n",
    "        # Create the filename\n",
    "        filename = f\"{channel_name}_{episode_number}.csv\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Create a new DataFrame with just the transcript\n",
    "        transcript_df = pd.DataFrame({'transcript': [transcript]})\n",
    "        \n",
    "        # Save the transcript as a CSV file\n",
    "        transcript_df.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"Saved: {filename}\")\n",
    "\n",
    "output_directory = \"../output/transcripts/\"\n",
    "save_transcripts_as_csv(df_videos, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "def summarize_with_bart(text, max_length=150, min_length=50):\n",
    "    # Check if MPS is available and set the device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        # Load pre-trained model and tokenizer\n",
    "        model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "        tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
    "        \n",
    "        # Move input tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate summary\n",
    "        summary_ids = model.generate(inputs['input_ids'],\n",
    "                                     num_beams=4,\n",
    "                                     max_length=max_length,\n",
    "                                     min_length=min_length,\n",
    "                                     length_penalty=2.0,\n",
    "                                     early_stopping=True)\n",
    "\n",
    "        # Decode the generated summary\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during summarization: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabled for now, summarising with Claude 3.5\n",
    "#summary = summarize_with_bart(df['transcript_chunks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "anthropic = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def summarize_transcript(transcript):\n",
    "    \"\"\"\n",
    "    Summarize a single transcript using Claude API\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Please summarize the following Fantasy Premier League podcast transcript. \n",
    "    Focus on the key points, player recommendations, and strategy advice.\n",
    "    Limit the summary to 3-5 bullet points.\n",
    "\n",
    "    Transcript:\n",
    "    {transcript}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    response = anthropic.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.5,\n",
    "        system=\"You are an expert in Fantasy Premier League and podcast summarization.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.content\n",
    "\n",
    "def process_transcripts(transcripts_dir):\n",
    "    \"\"\"\n",
    "    Process all transcripts in the given directory and group summaries by channel name\n",
    "    \"\"\"\n",
    "    summaries = defaultdict(list)\n",
    "\n",
    "    for filename in os.listdir(transcripts_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            channel_name = filename.split(\"_\")[0]\n",
    "            \n",
    "            with open(os.path.join(transcripts_dir, filename), \"r\") as file:\n",
    "                transcript = file.read()\n",
    "            \n",
    "            summary = summarize_transcript(transcript)\n",
    "            summaries[channel_name].append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "def save_summaries(summaries, output_file):\n",
    "    \"\"\"\n",
    "    Save the grouped summaries to a JSON file\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(summaries, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_dir = \"../output/transcripts/\"\n",
    "output_file = \"fpl_podcast_summaries.json\"\n",
    "\n",
    "summaries = process_transcripts(transcripts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"\n",
    "    Convert non-serializable objects to serializable format.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (str, int, float, bool, type(None))):\n",
    "        return obj\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        # For any other type, convert to string\n",
    "        return str(obj)\n",
    "\n",
    "def save_summaries(summaries, output_file):\n",
    "    \"\"\"\n",
    "    Save the grouped summaries to a JSON file\n",
    "    \"\"\"\n",
    "    serializable_summaries = convert_to_serializable(summaries)\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(serializable_summaries, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Summaries saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries saved to ../output/fpl_podcast_summaries.json\n"
     ]
    }
   ],
   "source": [
    "save_summaries(summaries, \"../output/fpl_podcast_summaries.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove TextBlock wrapper and unescape newlines\n",
    "    text = re.sub(r'TextBlock\\(text=|,\\s*type=\\'text\\'\\)', '', text)\n",
    "    text = text.strip(\"'\")\n",
    "    # Replace escaped newlines with actual newlines\n",
    "    text = text.replace('\\\\n', '\\n')\n",
    "    # Remove the introductory sentence\n",
    "    pattern = r\"Here's a summary of the key points from the Fantasy Premier League podcast transcript:[\\s\\n]*\"\n",
    "    text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    return text.strip()\n",
    "\n",
    "def is_summarizable(text):\n",
    "    unsummarizable_patterns = [\n",
    "        \"I apologize, but I cannot provide a meaningful summary\",\n",
    "        \"I'm sorry, but I can't provide a summary\",\n",
    "        \"Unable to provide a summary\"\n",
    "    ]\n",
    "    cleaned_text = clean_text(text).lower()\n",
    "    return not any(pattern.lower() in cleaned_text for pattern in unsummarizable_patterns)\n",
    "\n",
    "def filter_json(json_data):\n",
    "    filtered_data = {}\n",
    "    for channel, summaries in json_data.items():\n",
    "        filtered_summaries = [summary for summary in summaries if is_summarizable(summary[0])]\n",
    "        if filtered_summaries:\n",
    "            filtered_data[channel] = filtered_summaries\n",
    "    return filtered_data\n",
    "\n",
    "def json_to_markdown(json_data):\n",
    "    markdown = \"\"\n",
    "    \n",
    "    for channel, summaries in json_data.items():\n",
    "        markdown += f\"# {channel}\\n\\n\"\n",
    "        \n",
    "        for summary in summaries:\n",
    "            cleaned_summary = clean_text(summary[0])\n",
    "            # Add two spaces at the end of each line for Markdown line breaks\n",
    "            cleaned_summary = '\\n'.join(line.rstrip() + '  ' for line in cleaned_summary.split('\\n'))\n",
    "            markdown += cleaned_summary + \"\\n\\n\"\n",
    "        \n",
    "        markdown += \"---\\n\\n\"\n",
    "    \n",
    "    return markdown.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Markdown file 'fpl_podcast_summaries.md' has been created.\n",
      "Filtered JSON file 'filtered_fpl_podcast_summaries.json' has been created.\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open('../output/fpl_podcast_summaries.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter out unsummarizable transcripts\n",
    "filtered_data = filter_json(data)\n",
    "\n",
    "# Convert filtered JSON to Markdown\n",
    "markdown_content = json_to_markdown(filtered_data)\n",
    "\n",
    "# Write the Markdown content to a file\n",
    "with open('../output/fpl_podcast_summaries.md', 'w', encoding='utf-8') as file:\n",
    "    file.write(markdown_content)\n",
    "\n",
    "print(\"Conversion complete. Markdown file 'fpl_podcast_summaries.md' has been created.\")\n",
    "\n",
    "# Optionally, save the filtered JSON\n",
    "with open('../output/filtered_fpl_podcast_summaries.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(filtered_data, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Filtered JSON file 'filtered_fpl_podcast_summaries.json' has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_word_frequencies(text, min_length=1):\n",
    "    # Convert to lowercase and split into words\n",
    "    words = re.findall(r'\\b[\\w\\']+\\b', text.lower())\n",
    "    \n",
    "    # Filter words by minimum length if specified\n",
    "    if min_length > 1:\n",
    "        words = [word for word in words if len(word) >= min_length]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "# Combine all transcripts into a single string\n",
    "all_transcripts = ' '.join(df_videos['transcript'])\n",
    "\n",
    "# Get word frequencies\n",
    "word_frequencies = get_word_frequencies(all_transcripts)\n",
    "\n",
    "# Get the 100 most common words\n",
    "most_common_words = word_frequencies.most_common(100)\n",
    "\n",
    "# Print the results\n",
    "print(\"100 Most Common Words:\")\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
